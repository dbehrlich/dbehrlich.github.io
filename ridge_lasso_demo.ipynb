{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook I am going to demonstrate the ridge and lasso regression methods. I will show some numpy code for each method and how their performance compares on three types of datasets, uncorrelated predictors, collinear predictors and random data. Least squares and ridge regressions are performed analytically and the lasso regression is estimated numerically using the forward-stagewise algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ridge regression was invented to deal with collinearity among predictors, and lasso to deal with both shrinkage and model selection. In that case, as we will demonstrate below, if predictors are minimally correlated and all coefficients are substantial, the regression coefficients from the least squares, lasso and ridge regressions should be near identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Generate uncorrelated Data\n",
    "n=1000\n",
    "p=4\n",
    "sigma = .1\n",
    "z = 4*np.random.randn(n,p)\n",
    "y = 3*z[:,0] - z[:,1] + 2*z[:,2] + sigma*np.random.randn(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The least squares coefficients are: [  2.99952456e+00  -9.99992380e-01   2.00060114e+00   7.79235876e-05]\n"
     ]
    }
   ],
   "source": [
    "#Least squares\n",
    "beta_ls = np.dot(np.dot(np.linalg.inv(np.dot(z.T,z)),z.T),y)\n",
    "print 'The least squares coefficients are: {}'.format(beta_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ridge regression coefficients with lamda=2 are: [  2.99915701e+00  -9.99903414e-01   2.00036054e+00   8.33228918e-05]\n"
     ]
    }
   ],
   "source": [
    "#Ridge solution\n",
    "lam = 2\n",
    "beta_r = np.dot(np.dot(np.linalg.inv(np.dot(z.T,z)+lam*np.eye(np.shape(z)[1])),z.T),y)\n",
    "print 'The ridge regression coefficients with lamda={} are: {}'.format(lam,beta_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The lasso regression coefficients are: [ 3. -1.  2.  0.]\n"
     ]
    }
   ],
   "source": [
    "#Lasso solution (forward-stagewise algorithm)\n",
    "\n",
    "eps = .05\n",
    "iters = 10000\n",
    "res = np.zeros([np.shape(y)[0],iters])\n",
    "beta_l = np.zeros(np.shape(z)[1])\n",
    "res[:,0] = y\n",
    "\n",
    "for ii in range(iters):\n",
    "    pred=np.argmax( np.abs(np.corrcoef(np.column_stack([res[:,ii],z]).T)[1:,0]) ) #Most correlated predictor\n",
    "    delta_pred = eps*np.sign(np.dot(z[:,pred].T,res[:,ii]))\n",
    "    beta_l[pred] +=  delta_pred\n",
    "    if ii<iters-1:\n",
    "        res[:,ii+1] = res[:,ii] - delta_pred*z[:,pred] \n",
    "        \n",
    "print 'The lasso regression coefficients are: {}'.format(beta_l)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean squared error between:\n",
      "ridge and least square coefficients = 2.00925654194e-07\n",
      "lasso and least square coefficients = 5.93546292807e-07\n"
     ]
    }
   ],
   "source": [
    "#MSE between beta_ls and beta_r\n",
    "mse1=np.sum((beta_ls-beta_r)**2)\n",
    "mse2=np.sum((beta_ls-beta_l)**2)\n",
    "print 'The mean squared error between:\\nridge and least square coefficients = {}\\nlasso and least square coefficients = {}'.format(mse1,mse2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the ridge and lasso coefficients are nearly identical to our traditional least squares solution with the exception that lasso successfully zerod the non-generative predictor.\n",
    "Below, however, we are going to generate collinear predictors and we can see the effect that has on the three sets of regression coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Generate correlated Data\n",
    "z = 4*np.random.randn(n,p)\n",
    "z[:,2] = .5*z[:,1]+z[:,0]\n",
    "y = 3*z[:,0] - z[:,1] + 2*z[:,2] + sigma*np.random.randn(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The least squares coefficients are: [  4.83588071e+00  -8.63562085e-02   1.90852256e-02   8.99337678e-04]\n"
     ]
    }
   ],
   "source": [
    "#Least squares\n",
    "beta_ls = np.dot(np.dot(np.linalg.inv(np.dot(z.T,z)),z.T),y)\n",
    "print 'The least squares coefficients are: {}'.format(beta_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ridge regression coefficients with lamda=2 are: [  2.77707369e+00  -1.10901034e+00   2.22256852e+00   8.90635557e-04]\n"
     ]
    }
   ],
   "source": [
    "#Ridge solution\n",
    "lam = 2\n",
    "beta_r = np.dot(np.dot(np.linalg.inv(np.dot(z.T,z)+lam*np.eye(np.shape(z)[1])),z.T),y)\n",
    "print 'The ridge regression coefficients with lamda={} are: {}'.format(lam,beta_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The lasso regression coefficients are: [ 5.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "#Lasso solution (forward-stagewise algorithm)\n",
    "\n",
    "eps = .05\n",
    "iters = 10000\n",
    "res = np.zeros([np.shape(y)[0],iters])\n",
    "beta_l = np.zeros(np.shape(z)[1])\n",
    "res[:,0] = y\n",
    "\n",
    "for ii in range(iters):\n",
    "    pred=np.argmax( np.abs(np.corrcoef(np.column_stack([res[:,ii],z]).T)[1:,0]) ) #Most correlated predictor\n",
    "    delta_pred = eps*np.sign(np.dot(z[:,pred].T,res[:,ii]))\n",
    "    beta_l[pred] +=  delta_pred\n",
    "    if ii<iters-1:\n",
    "        res[:,ii+1] = res[:,ii] - delta_pred*z[:,pred] \n",
    "        \n",
    "print 'The lasso regression coefficients are: {}'.format(beta_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean squared error between:\n",
      "ridge and least square coefficients = 10.1398464555\n",
      "lasso and least square coefficients = 0.0347575901274\n"
     ]
    }
   ],
   "source": [
    "#MSE between beta_ls and beta_r\n",
    "mse1=np.sum((beta_ls-beta_r)**2)\n",
    "mse2=np.sum((beta_ls-beta_l)**2)\n",
    "print 'The mean squared error between:\\nridge and least square coefficients = {}\\nlasso and least square coefficients = {}'.format(mse1,mse2)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "In this second example, it is obvious that the three regression methods vary significantly. The least square solution came up with parameters poorly fit to the generative function, the lasso method created a sparse representation of the inaccurate least square estimates (it again correctly zerod the non-generative predictor), and only the ridge solution was able to accurately estimate the true coefficients (although it could not remove the non-generative fourth predictor). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Generate random Data\n",
    "\n",
    "z = 4*np.random.randn(n,p)\n",
    "y = sigma*np.random.randn(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The least squares coefficients are: [ 0.00086766 -0.00025277  0.00120982 -0.0004141 ]\n"
     ]
    }
   ],
   "source": [
    "#Least squares\n",
    "beta_ls = np.dot(np.dot(np.linalg.inv(np.dot(z.T,z)),z.T),y)\n",
    "print 'The least squares coefficients are: {}'.format(beta_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ridge regression coefficients with lamda=2 are: [ 0.00086754 -0.00025275  0.00120966 -0.00041403]\n"
     ]
    }
   ],
   "source": [
    "#Ridge solution\n",
    "lam = 2\n",
    "beta_r = np.dot(np.dot(np.linalg.inv(np.dot(z.T,z)+lam*np.eye(np.shape(z)[1])),z.T),y)\n",
    "print 'The ridge regression coefficients with lamda={} are: {}'.format(lam,beta_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The lasso regression coefficients are: [ 0.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "#Lasso solution (forward-stagewise algorithm)\n",
    "\n",
    "eps = .05\n",
    "iters = 10000\n",
    "res = np.zeros([np.shape(y)[0],iters])\n",
    "beta_l = np.zeros(np.shape(z)[1])\n",
    "res[:,0] = y\n",
    "\n",
    "for ii in range(iters):\n",
    "    pred=np.argmax( np.abs(np.corrcoef(np.column_stack([res[:,ii],z]).T)[1:,0]) ) #Most correlated predictor\n",
    "    delta_pred = eps*np.sign(np.dot(z[:,pred].T,res[:,ii]))\n",
    "    beta_l[pred] +=  delta_pred\n",
    "    if ii<iters-1:\n",
    "        res[:,ii+1] = res[:,ii] - delta_pred*z[:,pred] \n",
    "        \n",
    "print 'The lasso regression coefficients are: {}'.format(beta_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean squared error between:\n",
      "ridge and least square coefficients = 4.32071950523e-14\n",
      "lasso and least square coefficients = 2.45186527761e-06\n"
     ]
    }
   ],
   "source": [
    "#MSE between beta_ls and beta_r\n",
    "mse1=np.sum((beta_ls-beta_r)**2)\n",
    "mse2=np.sum((beta_ls-beta_l)**2)\n",
    "print 'The mean squared error between:\\nridge and least square coefficients = {}\\nlasso and least square coefficients = {}'.format(mse1,mse2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see above, the ridge and least squares coefficients are all small, but the methods are incapable of pushing them to zero. Only the lasso regression correctly determines that all coefficients are non-generative of y. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
